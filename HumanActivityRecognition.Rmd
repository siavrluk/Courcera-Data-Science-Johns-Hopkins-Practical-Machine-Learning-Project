---
title: "Human Activity Recognition"
author: "Silvana Avramska Lukarska"
date: "4/19/2021"
output: html_document
---


# Summary

In the last few years, the use of activity trackers has made the collection of activity performance data relatively inexpensive. One thing that people regularly do is quantify **how much** of a particular activity they do, but they rarely quantify **how well** they do it. In this project, our goal is to address the latter. We will be using data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information about the data can be found [here][har_data] under Weight Lifting Exercises Dataset. The training and test datasets for this project can be found [here][training_data] and [the test dataset [here][test_data], resp. 

After performing initial data exploration and cleaning, we will build a several predictive models. By comparing their accuracies, we concluded that the Random forest model with $300$ trees performs best and used it to obtain a prediction for our test set.

[har_data]: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har
[training_data]: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
[test_data]: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv


# Data processing 

## Exploration

```{r Load Data, results='hide', echo=FALSE, cache=TRUE}
train_df <- read.csv("pml-training.csv", skip=1, skipNul = TRUE)
test_df <- read.csv("pml-testing.csv", skip=1, skipNul = TRUE)

# remove first column
train_df <- train_df[, -1]
test_df <- test_df[, -1]

dim(train_df)
dim(test_df)

head(train_df)

str(train_df)

train_df$classe <- as.factor(train_df$classe)

# last 2 rows of the test set are empty
test_df <- head(test_df, -2)
```

The training data consists of $19 624$ observations of $159$ variables. The first 7 columns contain data about the subject's name and experiment setup, and the last column is the variable of interest, **classe**, accounting for the quality of the exercise performance. The rest of the columns contain sensor data and its summary. The test set contains 20 objects and $159$ variables. The first $158$ of them are the same as in the training set. The only difference is in the last column which this time contains **problem_id**. By looking at the training data, it seems there are several measurements recorded during a single repetition of an exercise (grouped in the same *window*). For each such repetition, the row where the value **new_widnow** is "yes" contains summary of all repetitions in the corresponding window. 

## Cleaning

Notice that there are many columns in the training set that contain more than 97% missing values. These are in fact all columns with summary data such as mean, standard deviation, variance, skewnees, and kurtosis of the different sensor data As mentioned above, they are only available when the value of **new_window** is yes, which is a very small fraction of all available data points. Since in the test data set we actually have just one sensor reading per participant, these summary columns are completely irrelevant and all their values are $NA$. So we will not use them for modeling. Furthermore, as mentioned above, the first 7 columns contain the participants' names and experiment setup. Since they are also irrelevant for the prediction, we will remove them as well. 

```{r Data cleaning, results='hide', echo=FALSE}
# percent of NA or empty string per column
head(sapply(train_df, function(x) mean((is.na(x) | x == ''))*100))

# drop columns with more than 90% missing
col_with_missing <- names(which(sapply(train_df, function(x) mean(is.na(x) | x == '') > 0.9)))

train_df_small <- train_df[, -which(names(train_df) %in% col_with_missing)]

# check again for missing values
sapply(train_df_small, function(x) which((is.na(x) | x == '')))
train_df_small[c(19623, 19624), ]
train_df_small <- train_df_small[-c(19623, 19624), ]

train_df_small$classe <- droplevels(train_df_small$classe)

# drop columns that bring no prediction power
col_to_drop <- c("user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")
train_df_clean <- train_df_small[, -which(names(train_df_small) %in% col_to_drop)]

dim(train_df_clean)
```

At the end, we are left with $53$ variables which we will use for modeling. 

```{r}
table(train_df_clean$classe)
```

We see that most of the classes are balanced, the only exception is classe **A** which has more than 1.5 times more observations. This should be addressed in case we don't have a satisfactory results from our model.

# Modeling

Due to the nature of the data, we decided to build 2 models -- one using Random forest, and a second, boosted model. In both cases, we start by splitting the data into *training* and *validation* sets, and will perform *5-fold cross-validation* to choose the best one. We do this in order to get an estimate of our model's error.

```{r split into training and validation sets, message=FALSE}
library(caret)

set.seed(443)
inTrain <- createDataPartition(y = train_df_clean$classe, p = 0.75, list = FALSE)
train_data <- train_df_clean[inTrain,]
val_data   <- train_df_clean[-inTrain,]

trControl <- trainControl(method="cv", number = 5, allowParallel = TRUE)
```


## Random Forest

We start by fitting a random forest model with $300$ trees.

```{r Random forest}
model_rf <- train(classe ~ ., data = train_data, method = 'rf', ntree = 300, trControl = trControl)
model_rf$finalModel
confusionMatrix(model_rf)

rf_pred <- predict(model_rf, newdata = val_data)
confusionMatrix(rf_pred, val_data$classe)
```

As we can see, both the in-sample ($99.07%$) and out-of-sample ($99.33%$) accuracy are quite high which make us confident that our model performs well. Note that in this case, we actually have a better out-of-sample accuracy which is a unusual but not impossible and it is due to the random splits in the cross-validation.

## Boosted Model

For our second model we will use Gradient boosted machine method. 

```{r Boosted, results='hide'}
model_boost <- train(classe ~ ., data = train_data, method = 'gbm', trControl = trControl, verbose = TRUE)
model_boost$finalModel
```

```{r}
confusionMatrix(model_boost)

boost_pred <- predict(model_boost, newdata = val_data)
confusionMatrix(boost_pred, val_data$classe)
```

Again, we get pretty good in-sample ($96.38%$) and out-of-sample ($95.8%$) accuracy. However, this model performs a little worse than the Random forest model.


# Predictions for the Quiz

As mentioned above, the Random forest model performs better than the boosted model so we will use it for making predictions for our test set. 

```{r Quiz}
pred_test <- predict(model_rf, newdata = test_df)
pred_test
```